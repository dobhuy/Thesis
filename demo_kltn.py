# -*- coding: utf-8 -*-
"""Demo_KLTN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rza_PzcYNNNcWqsn8N49qf8cDrv_UoZV
"""

from google.colab import drive
drive.mount('/content/drive')

# %%writefile app.py
# import torch
# import numpy as np
# import os
# import tensorflow as tf
# import pandas as pd
# import matplotlib.pyplot as plt
# import ast
# import streamlit as st
# import torch
# import torchaudio
# import pandas as pd
# import numpy as np
# import os
# from transformers import (
#     AutoTokenizer,
#     AutoModelForTokenClassification,
#     WhisperForConditionalGeneration,
#     WhisperProcessor,
#     AutoProcessor,
#     AutoModelForSpeechSeq2Seq,
#     Wav2Vec2ForCTC,
#     Wav2Vec2Processor,
#     )
# from typing import Any, Dict, List, Union
# import librosa
# import tempfile

# model_asr_path = "/content/phowhisper_base"
# model_name = os.path.basename(model_asr_path)
# if model_name.split('_')[0] == 'wav2vec2':
#   processor = Wav2Vec2Processor.from_pretrained(model_asr_path)
#   model_asr = Wav2Vec2ForCTC.from_pretrained(model_asr_path,ignore_mismatched_sizes=True)
# else:
#   processor = AutoProcessor.from_pretrained(model_asr_path)
#   model_asr = AutoModelForSpeechSeq2Seq.from_pretrained(model_asr_path)


# model_tsd_path = '/content/ViSoBERT'
# tokenizer = AutoTokenizer.from_pretrained(model_tsd_path)
# model_tsd = AutoModelForTokenClassification.from_pretrained(model_tsd_path, num_labels=2)


# def transcribe_with_phowhisper(audio_path, processor, model):
#     waveform, sr = librosa.load(audio_path, sr=16000)
#     inputs = processor(waveform, sampling_rate=16000, return_tensors="pt")
#     with torch.no_grad():
#         predicted_ids = model.generate(**inputs)
#     transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
#     return transcription

# def transcribe_with_whisper(audio_path, processor, model):
#   waveform, sample_rate = librosa.load(audio_path, sr=16000)
#   input_features = processor(waveform, return_tensors="pt", sampling_rate=16000).input_features

#   with torch.no_grad():
#       outputs = model.generate(
#           input_features,
#           forced_decoder_ids=processor.get_decoder_prompt_ids(language="vi", task="transcribe"),
#           return_dict_in_generate=True
#       )
#   transcription = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0]
#   return transcription

# def transcribe_with_wav2vec2(audio_path, processor, model):
#   waveform, sample_rate = librosa.load(audio_path, sr=16000)

#   # Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh tensor
#   input_values = processor(waveform, sampling_rate=16000, return_tensors="pt").input_values

#   # D·ª± ƒëo√°n logits
#   with torch.no_grad():
#       logits = model(input_values).logits
#   predicted_ids = torch.argmax(logits, dim=-1)

#   # Decode th√†nh vƒÉn b·∫£n
#   transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)
#   return transcription


# def detect_toxic_spans(transcript, tokenizer, model):
#   max_len=len(list(transcript))
#   enc = tokenizer(list([transcript]), is_split_into_words=True, padding='max_length', truncation=True, max_length = max_len, return_tensors="pt")
#   with torch.no_grad():
#     logits = model(input_ids=enc.input_ids, attention_mask=enc.attention_mask).logits
#   pred_ids = logits.argmax(-1)[0].cpu().tolist()
#   return pred_ids

# def highlight_toxic_span(text, labels):
#     highlighted_text = ""
#     for char, label in zip(text, labels):
#         if label == 1:
#             highlighted_text += f"<mark style='background-color:#ff6b6b'>{char}</mark>"
#         else:
#             highlighted_text += char
#     return highlighted_text


# st.set_page_config(page_title="AUDIO BASED TOXIC SPANS DETECTION ON VIETNAMESE SPEECH UTTERANCES", layout="centered")
# st.markdown("""
#     <style>
#     .center {
#         display: flex;
#         flex-direction: column;
#         justify-content: center;
#         align-items: center;
#         text-align: center;
#         margin-top: 80px;
#     }
#     .upload-btn {
#         background-color: #dce3f6;
#         color: #000;
#         padding: 12px 30px;
#         border-radius: 30px;
#         font-weight: bold;
#         margin-top: 20px;
#         cursor: pointer;
#     }
#     </style>
# """, unsafe_allow_html=True)
# if "step" not in st.session_state:
#     st.session_state.step = "upload"

# def show_upload_screen():
#     st.title("AUDIO BASED TOXIC SPANS DETECTION ON VIETNAMESE SPEECH UTTERANCES")
#     st.markdown('<div class="center">', unsafe_allow_html=True)
#     st.markdown("## Upload an audio file you want to detect toxic spans", unsafe_allow_html=True)

#     uploaded_file = st.file_uploader("", type=["wav"], label_visibility="collapsed")



#     if uploaded_file:
#       st.markdown("## Upload Ground-truth of the audio file", unsafe_allow_html=True)

#       uploaded_file = st.file_uploader("", type=["wav"], label_visibility="collapsed")

#     st.markdown('<div class="upload-btn">Upload Audio File Now</div>', unsafe_allow_html=True)
#     st.markdown('</div>', unsafe_allow_html=True)


# def show_results():
#     st.subheader("‚úÖ Audio Uploaded Successfully!")
#     st.audio(st.session_state.audio_path)

#     with st.spinner("üîä Transcribing..."):
#         transcript = transcribe_with_phowhisper(st.session_state.audio_path,processor,model_asr)
#     st.success("Transcript complete!")

#     st.subheader("üìù Transcript:")
#     st.write(transcript)

#     with st.spinner("‚ö†Ô∏è Detecting toxic spans..."):
#         pred_ids = detect_toxic_spans(transcript, tokenizer, model_tsd)
#         highlighted_html = highlight_toxic_span(transcript.split(), pred_ids)

#     st.subheader("üö´ Toxic Spans Highlighted:")
#     st.markdown(highlighted_html, unsafe_allow_html=True)

#     if st.button("üîÑ Upload Another File"):
#         st.session_state.step = "upload"
#         st.experimental_rerun()

# if st.session_state.step == "upload":
#     show_upload_screen()
# elif st.session_state.step == "process":
#     show_results()

# %%writefile app.py
# import os
# import tempfile
# import streamlit as st
# import librosa
# import torch
# import pandas as pd
# from transformers import (
#     Wav2Vec2Processor, Wav2Vec2ForCTC,
#     WhisperProcessor, WhisperForConditionalGeneration,
#     AutoTokenizer, AutoModelForTokenClassification
# )

# # --- Configuration: model paths ---
# ASR_MODELS = {
#     "PhoWhisper": "/content/drive/MyDrive/KLTN/ASR_models/phowhisper_base",
#     # "OpenAI Whisper": "/content/whisper_base",
#     "Wav2Vec2": "/content/drive/MyDrive/KLTN/ASR_models/wa2vec2_vlsp_37WER",
# }
# TSD_MODELS = {
#     "PhoBERT": "/content/drive/MyDrive/KLTN/TSD_models/phobert_base_v2",
#     "ViSoBERT": "/content/drive/MyDrive/KLTN/TSD_models/ViSoBERT",
#     "CafeBERT": "/content/drive/MyDrive/KLTN/TSD_models/cafebert",
#     "XLMR": "/content/drive/MyDrive/KLTN/TSD_models/xlmr",
# }

# # --- Load ASR processors & models ---
# asr_processors = {}
# asr_models = {}
# for name, path in ASR_MODELS.items():
#     lower = os.path.basename(path).lower()
#     if 'wav2vec2' in lower:
#         proc = Wav2Vec2Processor.from_pretrained(path)
#         mod = Wav2Vec2ForCTC.from_pretrained(path, ignore_mismatched_sizes=True)
#     elif 'whisper' in lower or 'phowhisper' in lower:
#         proc = WhisperProcessor.from_pretrained(path)
#         mod = WhisperForConditionalGeneration.from_pretrained(path)
#     else:
#         st.error(f"Unsupported ASR model type for {name}")
#         continue
#     asr_processors[name] = proc
#     asr_models[name] = mod

# # --- Load TSD tokenizers & models ---
# tsd_tokenizers = {}
# tsd_models = {}
# for name, path in TSD_MODELS.items():
#     tok = AutoTokenizer.from_pretrained(path)
#     mod = AutoModelForTokenClassification.from_pretrained(path, num_labels=2)
#     tsd_tokenizers[name] = tok
#     tsd_models[name] = mod

# st.markdown(
#     """
#     <style>
#     div.background {
#         background-color: 889ECE;

#         display: flex;
#         flex-direction: column;
#         justify-content: center;
#         align-items: center;
#         text-align: center;
#         margin-top: 80px;
#     }
#     </style>
#     """,
#     unsafe_allow_html=True
# )
# # --- Streamlit UI ---
# st.title("Toxic Span Detection from Audio Transcript")
# # CSS for red button
# st.markdown(
#     """
#     <style>
#     div.stButton > button:first-child {
#         background-color: red !important;
#         color: white !important;
#         border: none;
#     }
#     </style>
#     """,
#     unsafe_allow_html=True
# )

# # Step 1: Upload audio
# uploaded_audio = st.file_uploader("1. Upload a WAV audio file", type=["wav"])
# if not uploaded_audio:
#     st.info("Please upload a WAV audio file to begin.")
#     st.stop()
# # save temp
# tfile = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
# tfile.write(uploaded_audio.read()); tfile.flush()
# audio_path = tfile.name
# st.success("Audio uploaded.")
# st.audio(audio_path, format='audio/wav')

# # Step 2: Upload ground-truth transcript
# uploaded_txt = st.file_uploader("2. Upload ground-truth transcript (TXT)", type=["txt"])
# if not uploaded_txt:
#     st.info("Please upload the transcript TXT file.")
#     st.stop()
# transcript_text = uploaded_txt.read().decode('utf-8').strip()

# # Step 3: Run ASR and TSD
# if st.button("Transcript and Detect Toxic Spans Now"):
#     # --- Section 1: ASR table ---
#     st.subheader("ASR Transcriptions")
#     rows = []
#     # Groundtruth
#     rows.append({"Model": "Groundtruth", "Transcript": transcript_text})
#     # Automatic models
#     for name, proc in asr_processors.items():
#         mod = asr_models[name]
#         waveform, _ = librosa.load(audio_path, sr=16000)
#         if isinstance(proc, Wav2Vec2Processor):
#             ins = proc(waveform, sampling_rate=16000, return_tensors="pt")
#             with torch.no_grad(): pred = mod(ins.input_values).logits
#             pred_ids = torch.argmax(pred, dim=-1)
#             text = proc.decode(pred_ids[0], skip_special_tokens=True)
#         else:  # WhisperProcessor
#             ins = proc(waveform, return_tensors="pt", sampling_rate=16000)
#             with torch.no_grad():
#                 outs = mod.generate(
#                     ins.input_features,
#                     forced_decoder_ids=proc.get_decoder_prompt_ids(language="vi", task="transcribe"),
#                     return_dict_in_generate=True
#                 )
#             text = proc.batch_decode(outs.sequences, skip_special_tokens=True)[0]
#         rows.append({"Model": name, "Transcript": text})
#     df_asr = pd.DataFrame(rows)
#     st.table(df_asr)

#     # --- Section 2: Toxic Span Detection ---
#     st.subheader("Toxic Span Detection Results")
#     # HTML table header
#     html = ["<table><tr><th>Model</th><th>Result</th></tr>"]
#     def highlight_toxic_span(text, labels):
#         out = ""
#         for c, l in zip(text, labels):
#             if l == 1:
#                 out += f"<mark style='background-color:#ff6b6b'>{c}</mark>"
#             else:
#                 out += c
#         return out

#     for name, tok in tsd_tokenizers.items():
#         mod = tsd_models[name]
#         enc = tok([transcript_text], is_split_into_words=True,
#                   padding='max_length', truncation=True,
#                   max_length=len(transcript_text), return_tensors="pt")
#         with torch.no_grad(): logits = mod(input_ids=enc.input_ids, attention_mask=enc.attention_mask).logits
#         labels = logits.argmax(-1)[0].cpu().tolist()
#         highlighted = highlight_toxic_span(transcript_text, labels)
#         html.append(f"<tr><td>{name}</td><td>{highlighted}</td></tr>")
#     html.append("</table>")
#     st.markdown('\n'.join(html), unsafe_allow_html=True)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import os
# import tempfile
# import streamlit as st
# import librosa
# import torch
# import pandas as pd
# from transformers import (
#     AutoConfig,
#     Wav2Vec2Processor, Wav2Vec2ForCTC,
#     WhisperProcessor, WhisperForConditionalGeneration,
#     AutoTokenizer, AutoModelForTokenClassification
# )
# 
# # --- Configuration: model paths ---
# ASR_MODELS = {
#     "PhoWhisper": "/content/drive/MyDrive/KLTN/ASR_models/phowhisper_base",
#     # "OpenAI Whisper": "/content/whisper_base",
#     "Wav2Vec2": "/content/drive/MyDrive/KLTN/ASR_models/wa2vec2_vlsp_37WER",
# }
# TSD_MODELS = {
#     "PhoBERT": "/content/drive/MyDrive/KLTN/TSD_models/phobert_base_v2",
#     "ViSoBERT": "/content/drive/MyDrive/KLTN/TSD_models/ViSoBERT",
#     "CafeBERT": "/content/drive/MyDrive/KLTN/TSD_models/cafebert",
#     "XLMR": "/content/drive/MyDrive/KLTN/TSD_models/xlmr",
#     "BERT": "/content/drive/MyDrive/KLTN/TSD_models/BERT",
#     "DistilBERT": "/content/drive/MyDrive/KLTN/TSD_models/DistilBERT",
# }
# 
# # --- Load ASR processors & models using config ---
# asr_processors = {}
# asr_models = {}
# for name, path in ASR_MODELS.items():
#     config = AutoConfig.from_pretrained(path)
#     if config.model_type == "wav2vec2":
#         proc = Wav2Vec2Processor.from_pretrained(path)
#         mod = Wav2Vec2ForCTC.from_pretrained(path, ignore_mismatched_sizes=True)
#         # st.write(f"Loaded Wav2Vec2 ASR for {name}")
#     elif config.model_type in ["whisper", "phowhisper"]:
#         proc = WhisperProcessor.from_pretrained(path)
#         mod = WhisperForConditionalGeneration.from_pretrained(path)
#         # st.write(f"Loaded Whisper ASR for {name}")
#     else:
#         st.error(f"Kh√¥ng h·ªó tr·ª£ lo·∫°i ASR model {config.model_type} t·∫°i {name}")
#         continue
#     asr_processors[name] = proc
#     asr_models[name] = mod
# 
# # --- Load TSD tokenizers & models ---
# tsd_tokenizers = {}
# tsd_models = {}
# for name, path in TSD_MODELS.items():
#     tok = AutoTokenizer.from_pretrained(path)
#     mod = AutoModelForTokenClassification.from_pretrained(path, num_labels=2)
#     tsd_tokenizers[name] = tok
#     tsd_models[name] = mod
# 
# # --- Streamlit UI ---
# # CSS for animated background gradient and styled button
# st.markdown(
#     """
#     <style>
#     @keyframes bgfade {
#         0% { background-color: white; }
#         100% { background-color: white; }
#         50% { background-color: #889ECE; }
#     }
#     html, body, .reportview-container, .main {
#         height: 100% !important;
#         margin: 0;
#         padding: 0;
#         animation: bgfade 10s ease infinite;
#     }
#     div.stButton > button:first-child {
#         background-color: red !important;
#         color: white !important;
#         border: none;
#     }
#     </style>
#     """,
#     unsafe_allow_html=True
# )
# 
# st.title("Toxic Span Detection from Audio Transcript")
# 
# # Step 1: Upload audio
# uploaded_audio = st.file_uploader("1. Upload a WAV audio file", type=["wav"])
# if not uploaded_audio:
#     st.info("Please upload a WAV audio file to begin.")
#     st.stop()
# with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tfile:
#     tfile.write(uploaded_audio.read())
#     audio_path = tfile.name
# st.success("Audio uploaded.")
# st.audio(audio_path, format='audio/wav')
# 
# # Step 2: Upload ground-truth transcript
# uploaded_txt = st.file_uploader("2. Upload ground-truth transcript (TXT)", type=["txt"])
# if not uploaded_txt:
#     st.info("Please upload the transcript TXT file.")
#     st.stop()
# transcript_text = uploaded_txt.read().decode('utf-8').strip()
# 
# # Step 3: Run ASR and TSD
# if st.button("Transcript and Detect Toxic Spans Now"):
#     # Section 1: ASR transcriptions table
#     st.subheader("ASR Transcriptions")
#     rows = [{"Model": "Groundtruth", "Transcript": transcript_text}]
#     for name, proc in asr_processors.items():
#         mod = asr_models[name]
#         waveform, _ = librosa.load(audio_path, sr=16000)
#         if isinstance(proc, Wav2Vec2Processor):
#             inputs = proc(waveform, sampling_rate=16000, return_tensors="pt")
#             with torch.no_grad(): logits = mod(inputs.input_values).logits
#             pred_ids = torch.argmax(logits, dim=-1)
#             text = proc.decode(pred_ids[0], skip_special_tokens=True)
#         else:
#             inputs = proc(waveform, return_tensors="pt", sampling_rate=16000)
#             with torch.no_grad():
#                 outs = mod.generate(
#                     inputs.input_features,
#                     forced_decoder_ids=proc.get_decoder_prompt_ids(language="vi", task="transcribe"),
#                     return_dict_in_generate=True
#                 )
#             text = proc.batch_decode(outs.sequences, skip_special_tokens=True)[0]
#         rows.append({"Model": name, "Transcript": text})
#     st.table(pd.DataFrame(rows))
# 
#     # Section 2: Toxic span detection table
#     st.subheader("Toxic Span Detection Results")
#     html = [
#         "<table style='width:100%; border-collapse: collapse;'>",
#         "<tr><th style='border:1px solid #ddd; padding:8px;'>Model</th>",
#         "<th style='border:1px solid #ddd; padding:8px;'>Result</th></tr>"
#     ]
#     def highlight_toxic_span(text, labels):
#         out = ""
#         for c, l in zip(text, labels):
#             if l:
#                 out += f"<mark style='background-color:#ff6b6b'>{c}</mark>"
#             else:
#                 out += c
#         return out
#     for name, tok in tsd_tokenizers.items():
#         mod = tsd_models[name]
#         enc = tok([transcript_text], is_split_into_words=True,
#                   padding='max_length', truncation=True,
#                   max_length=len(transcript_text), return_tensors="pt")
#         with torch.no_grad(): logits = mod(input_ids=enc.input_ids, attention_mask=enc.attention_mask).logits
#         labels = logits.argmax(-1)[0].cpu().tolist()
#         highlighted = highlight_toxic_span(transcript_text, labels)
#         html.append(
#             f"<tr><td style='border:1px solid #ddd; padding:8px;'>{name}</td>"
#             f"<td style='border:1px solid #ddd; padding:8px;'>{highlighted}</td></tr>"
#         )
#     html.append("</table>")
#     st.markdown('\n'.join(html), unsafe_allow_html=True)
#

# @title Setup code
!pip install -q streamlit
!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64
!chmod +x cloudflared-linux-amd64
# import subprocess
# subprocess.Popen(["./cloudflared-linux-amd64", "tunnel", "--url", "http://localhost:8501"])
!nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &

!grep -o 'https://.*\.trycloudflare.com' nohup.out | head -n 1 | xargs -I {} echo "Your tunnel url {}"

!streamlit run /content/app.py &>/content/logs.txt &

